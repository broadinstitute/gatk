//Note: this section 'buildscript` is only for the dependencies of the buildscript itself.
// See the second 'repositories' section below for the actual dependencies of GATK itself
buildscript {
    repositories {
        mavenCentral()
        jcenter() // for shadow plugin
     }
}


plugins {
    id "java"           // set up default java compile and test tasks
    id "application"    // provides installDist
    id 'maven'          // record code coverage during test execution
    id 'signing'
    id "jacoco"
    id "de.undercouch.download" version "2.1.0" //used for downloading GSA lib
    id "com.github.johnrengelman.shadow" version "1.2.3"    //used to build the shadow and sparkJars
    id "com.github.kt3k.coveralls" version "2.6.3"  // report coverage data to coveralls
    id "com.github.ben-manes.versions" version "0.12.0" //used for identifying dependencies that need updating
    id 'com.palantir.git-version' version '0.5.1' //version helper
}
    
import com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar
import de.undercouch.gradle.tasks.download.Download
import javax.tools.ToolProvider;

mainClassName = "org.broadinstitute.hellbender.Main"

//Note: the test suite must use the same defaults. If you change system properties in this list you must also update the one in the test task
applicationDefaultJvmArgs = ["-Dsamjdk.use_async_io_read_samtools=false","-Dsamjdk.use_async_io_write_samtools=true", "-Dsamjdk.use_async_io_write_tribble=false", "-Dsamjdk.compression_level=1", "-Dsnappy.disable=true"]

//Delete the windows script - we never test on Windows so let's not pretend it works
startScripts {
    doLast {
        delete windowsScript
    }
}

task downloadGsaLibFile(type: Download) {
    src 'http://cran.r-project.org/src/contrib/gsalib_2.1.tar.gz'
    dest "src/main/resources/org/broadinstitute/hellbender/utils/R/gsalib.tar.gz"
    overwrite false
}


repositories {
    mavenCentral()
    jcenter()

    maven {
        url "https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/" //for htsjdk snapshots
    }

    mavenLocal()
}

final htsjdkVersion = System.getProperty('htsjdk.version','2.11.0-4-g958dc6e-SNAPSHOT')
final sparkVersion = System.getProperty('spark.version', '2.0.2')
final hadoopBamVersion = System.getProperty('hadoopBam.version','7.9.0')
final genomicsdbVersion = System.getProperty('genomicsdb.version','0.6.4-proto-3.0.0-beta-1')
final testNGVersion = '6.11'

final baseJarName = 'gatk'
final secondaryBaseJarName = 'hellbender'

configurations.all {
    resolutionStrategy {
        force 'com.google.http-client:google-http-client:1.21.0'
        // the snapshot folder contains a dev version of guava, we don't want to use that.
        force 'com.google.guava:guava:18.0'
        // force the htsjdk version so we don't get a different one transitively
        force 'com.github.samtools:htsjdk:' + htsjdkVersion
        // later versions explode Hadoop
        force 'com.google.protobuf:protobuf-java:3.0.0-beta-1'
        // force testng dependency so we don't pick up a different version via GenomicsDB
        force 'org.testng:testng:' + testNGVersion
    }
    all*.exclude group: 'org.slf4j', module: 'slf4j-jdk14' //exclude this to prevent slf4j complaining about to many slf4j bindings
    all*.exclude group: 'com.google.guava', module: 'guava-jdk5'
}


jacocoTestReport {
    dependsOn test
    group = "Reporting"
    description = "Generate Jacoco coverage reports after running tests."
    additionalSourceDirs = files(sourceSets.main.allJava.srcDirs)

    reports {
        xml.enabled = true // coveralls plugin depends on xml format report
        html.enabled = true
    }
}

//NOTE: we ignore contracts for now
compileJava {
  options.compilerArgs = ['-proc:none', '-Xlint:all','-Werror', '-Xdiags:verbose']
}
compileTestJava {
  options.compilerArgs = ['-proc:none', '-Xlint:all','-Werror', '-Xdiags:verbose']
}

// Get the jdk files we need to run javaDoc. We need to use these during compile, testCompile,
// test execution, and gatkDoc generation, but we don't want them as part of the runtime
// classpath and we don't want to redistribute them in the uber jar.
final javadocJDKFiles = files(((URLClassLoader) ToolProvider.getSystemToolClassLoader()).getURLs())

dependencies {
    // javadoc utilities; compile/test only to prevent redistribution of sdk jars
    compileOnly(javadocJDKFiles)
    testCompile(javadocJDKFiles)

    compile 'org.broadinstitute:barclay:1.2.1'
    compile ('com.intel:genomicsdb:' + genomicsdbVersion)  {
        exclude module: 'spark-core_2.10'
        exclude module: 'htsjdk'
        exclude module: 'protobuf-java'
    }
    compile 'com.opencsv:opencsv:3.4'
    compile 'com.google.guava:guava:18.0'
    compile 'com.github.samtools:htsjdk:'+ htsjdkVersion
    // Using the shaded version to avoid conflicts between its protobuf dependency
    // and that of Hadoop/Spark (either the one we reference explicitly, or the one
    // provided by dataproc).
    compile 'com.google.cloud:google-cloud-nio:0.20.4-alpha-20170727.190814-1:shaded'
    compile 'com.google.cloud.genomics:google-genomics-dataflow:v1beta2-0.15'
    compile 'com.google.cloud.genomics:gatk-tools-java:1.1'
    // this comes built-in when running on Google Dataproc, but the library
    // allows us to read from GCS also when testing locally (or on non-Dataproc clusters,
    // should we want to)
    compile 'com.google.cloud.bigdataoss:gcs-connector:1.6.1-hadoop2'
    compile 'org.apache.logging.log4j:log4j-api:2.3'
    compile 'org.apache.logging.log4j:log4j-core:2.3'
    compile 'org.apache.commons:commons-lang3:3.4'
    compile 'org.apache.commons:commons-math3:3.5'
    compile 'org.apache.commons:commons-collections4:4.1'
    compile 'org.apache.commons:commons-vfs2:2.0'
    compile 'commons-io:commons-io:2.4'
    compile 'org.reflections:reflections:0.9.10'
    compile 'com.google.cloud.dataflow:google-cloud-dataflow-java-sdk-all:0.4.150727'
    compile 'it.unimi.dsi:fastutil:7.0.6'
    compile 'com.google.apis:google-api-services-genomics:v1-rev90-1.22.0'
    compile 'com.google.cloud.genomics:google-genomics-utils:v1beta2-0.30'
    compile 'com.github.wendykierp:JTransforms:3.1'
    compile 'org.broadinstitute:hdf5-java-bindings:1.1.0-hdf5_2.11.0'
    compile 'org.nd4j:nd4j-native-platform:0.5.0'
    compile 'org.nd4j:nd4j-kryo_2.11:0.5.0'

    compile 'org.ojalgo:ojalgo:39.0'
    compile ('org.apache.spark:spark-mllib_2.11:' + sparkVersion) {
        // JUL is used by Google Dataflow as the backend logger, so exclude jul-to-slf4j to avoid a loop
        exclude module: 'jul-to-slf4j'
        exclude module: 'javax.servlet'
        exclude module: 'servlet-api'
    }

    compile 'org.bdgenomics.bdg-formats:bdg-formats:0.5.0'
    compile('org.bdgenomics.adam:adam-core-spark2_2.11:0.20.0') {
        exclude group: 'org.slf4j'
        exclude group: 'org.apache.hadoop'
        exclude group: 'org.scala-lang'
        exclude module: 'kryo'
        exclude module: 'hadoop-bam'
    }

    compile 'org.jgrapht:jgrapht-core:0.9.1'
    compile 'org.testng:testng:' + testNGVersion //compile instead of testCompile because it is needed for test infrastructure that needs to be packaged
    compile 'org.apache.hadoop:hadoop-minicluster:2.7.2' //the version of minicluster should match the version of hadoop

    compile('org.seqdoop:hadoop-bam:' + hadoopBamVersion) {
        exclude group: 'org.apache.hadoop'
        exclude module: 'htsjdk'
    }
    compile('org.apache.hadoop:hadoop-client:2.7.2') // should be a 'provided' dependency
    compile('com.github.jsr203hadoop:jsr203hadoop:1.0.3')

    compile('de.javakaffee:kryo-serializers:0.41') {
        exclude module: 'kryo' // use Spark's version
    }

    // Dependency change for including MLLib
    compile('org.objenesis:objenesis:1.2')
    testCompile('org.objenesis:objenesis:2.1')

    // Comment the next line to disable native code proxies in Spark MLLib
    compile('com.github.fommil.netlib:all:1.1.2')

    // Dependency change for including MLLib
    compile('com.esotericsoftware:kryo:3.0.3'){
        exclude group: 'com.esotericsoftware', module: 'reflectasm'
        exclude group: 'org.ow2.asm', module: 'asm'
    }

    // Dependency change for including MLLib
    compile('com.esotericsoftware:reflectasm:1.10.0:shaded')

    compile('com.intel.gkl:gkl:0.5.8') {
        exclude module: 'htsjdk'
    }

    compile 'org.broadinstitute:gatk-bwamem-jni:1.0.2'
    compile 'org.broadinstitute:gatk-fermilite-jni:1.0.0'

    //needed for DataflowAssert
    testCompile 'org.hamcrest:hamcrest-all:1.3'
    testCompile 'junit:junit:4.12'
    testCompile "org.mockito:mockito-core:1.10.19"
}

//add gatk-launch to the jar as a resource
processResources {
    from("gatk-launch")
}

processTestResources {
    //Don't waste time packaging our test data into the test jar
    exclude "**/large/**"
    exclude "**/org/broadinstitute/hellbender/**"
}

sourceCompatibility = 1.8
targetCompatibility = 1.8

def createSymlinks(archivePath, symlinkLocation) {
    exec {
        commandLine 'ln', '-fs', archivePath, symlinkLocation
        ignoreExitValue = false
    }
}

// Suffix is what will be added to the symlink
def createGatkSymlinks(destinationDir, archivePath, suffix, baseJarName, secondaryBaseJarName) {
    def finalSuffix = (suffix == "") ? "" : ("-" + suffix)

    def symlinkLocation = destinationDir.toString() + "/" + baseJarName + finalSuffix + ".jar"
    def symlinkLocation2 = destinationDir.toString() + "/" + secondaryBaseJarName + finalSuffix + ".jar"

    createSymlinks(archivePath.getAbsolutePath(), symlinkLocation)
    createSymlinks(archivePath.getAbsolutePath(), symlinkLocation2)
}

final isRelease = Boolean.getBoolean("release")
version = (isRelease ? gitVersion() : gitVersion() + "-SNAPSHOT").replaceAll(".dirty", "")

logger.info("build for version:" + version)
group = 'org.broadinstitute'


tasks.withType(Jar) {
    manifest {
        attributes 'Implementation-Title': 'Hellbender-tools',
                'Implementation-Version': version,
                'Main-Class': project.mainClassName
    }
}

test {
    outputs.upToDateWhen { false }  //tests will never be "up to date" so you can always rerun them
    String TEST_VERBOSITY = "$System.env.TEST_VERBOSITY"

    /**
     * Valid options for TEST_TYPE are:
     * cloud, integration, unit  : run one of the three disjoint partitions of the test suite
     * all                       : run all the tests
     * anything else             : run the non-cloud tests
     */
    String TEST_TYPE = "$System.env.TEST_TYPE"

    useTestNG {
        if (TEST_TYPE == "cloud") {
            // run only the cloud tests
            includeGroups 'cloud', 'bucket'
        } else if (TEST_TYPE == "integration"){
            include "**/*IntegrationTest.class"
            excludeGroups "cloud", "bucket"
        } else if (TEST_TYPE == "unit") {
            exclude "**/*IntegrationTest.class"
            excludeGroups "cloud", "bucket"
        } else if (TEST_TYPE == "spark") {
            includeGroups "spark"
            excludeGroups "cloud", "bucket"
        } else if (TEST_TYPE == "all"){
            //include everything
        } else {
            excludeGroups "cloud", "bucket"
        }
    }

    systemProperty "samjdk.use_async_io_read_samtools", "false"
    systemProperty "samjdk.use_async_io_write_samtools", "true"
    systemProperty "samjdk.use_async_io_write_tribble", "false"
    systemProperty "samjdk.compression_level", "1"
    systemProperty "snappy.disable", "true"
    systemProperty "gatk.spark.debug", System.getProperty("gatk.spark.debug")

    // Nd4j
    systemProperty "dtype", "double"
    environment "OMP_NUM_THREADS", "4"

    environment "SPARK_LOCAL_IP","127.0.0.1"

    // set heap size for the test JVM(s)
    minHeapSize = "1G"
    maxHeapSize = "4G"

    if (TEST_VERBOSITY == "minimal") {
        int count = 0
        // listen to events in the test execution lifecycle

        beforeTest { descriptor ->
            count++
            if( count % 10000 == 0) {
                logger.lifecycle("Finished "+ Integer.toString(count++) + " tests")
            }
        }
    } else {
        // show standard out and standard error of the test JVM(s) on the console
        testLogging.showStandardStreams = true
        beforeTest { descriptor ->
            logger.lifecycle("Running Test: " + descriptor)
        }

        // listen to standard out and standard error of the test JVM(s)
        onOutput { descriptor, event ->
            logger.lifecycle("Test: " + descriptor + " produced standard out/err: " + event.message )
        }
    }

    testLogging {
        testLogging {
            events "skipped", "failed"
            exceptionFormat = "full"
        }
        afterSuite { desc, result ->
            if (!desc.parent) { // will match the outermost suite
                println "Results: ${result.resultType} (${result.testCount} tests, ${result.successfulTestCount} successes, ${result.failedTestCount} failures, ${result.skippedTestCount} skipped)"
            }
        }
    }
}


task wrapper(type: Wrapper) {
    gradleVersion = '3.1'
}

tasks.withType(ShadowJar) {
    from(project.sourceSets.main.output)
    baseName = project.name + '-package'
    mergeServiceFiles()
    relocate 'com.google.common', 'org.broadinstitute.hellbender.relocated.com.google.common'
    zip64 true
    exclude 'log4j.properties' // from adam jar as it clashes with hellbender's log4j2.xml
    exclude '**/*.SF' // these are Manifest signature files and
    exclude '**/*.RSA' // keys which may accidentally be imported from other signed projects and then fail at runtime

    // Suggested by the akka devs to make sure that we do not get the spark configuration error.
    // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler
    transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {
        resource = 'reference.conf'
    }
}



// Dependency change for including MLLib
configurations {
    compile.exclude module: 'jul-to-slf4j'
    compile.exclude module: 'javax.servlet'
    compile.exclude module: 'servlet-api'
    compile.exclude group: 'com.esotericsoftware.kryo'

    sparkConfiguration {
        extendsFrom runtime
        // exclude Hadoop and Spark dependencies, since they are provided when running with Spark
        // (ref: http://unethicalblogger.com/2015/07/15/gradle-goodness-excluding-depends-from-shadow.html)
        exclude group: 'org.apache.hadoop'
        exclude module: 'spark-core_2.11'
        exclude group: 'org.slf4j'
        exclude module: 'jul-to-slf4j'
        exclude module: 'javax.servlet'
        exclude module: 'servlet-api'
        exclude group: 'com.esotericsoftware.kryo'
        exclude module: 'spark-mllib_2.11'
        exclude group: 'org.scala-lang'
        exclude module: 'kryo'
    }
}

shadowJar {
    configurations = [project.configurations.runtime]
    classifier = 'local'
    mergeServiceFiles('reference.conf')
    doLast {
        // Create a symlink to the newly created jar.  The name will be gatk.jar and
        //  it will be at the same level as the newly created jar.  (overwriting symlink, if it exists)
        // Please note that this will cause failures in Windows, which does not support symlinks.
        createGatkSymlinks(destinationDir.toString(), archivePath, "", baseJarName, secondaryBaseJarName)
    }
}

task localJar{ dependsOn shadowJar }

task sparkJar(type: ShadowJar) {
    group = "Shadow"
    description = "Create a combined jar of project and runtime dependencies that excludes provided spark dependencies"
    configurations = [project.configurations.sparkConfiguration]
    classifier = 'spark'
    doLast {
        // Create a symlink to the newly created jar.  The name will be gatk.jar and
        //  it will be at the same level as the newly created jar.  (overwriting symlink, if it exists)
        // Please note that this will cause failures in Windows, which does not support symlinks.
        createGatkSymlinks(destinationDir.toString(), archivePath, classifier, baseJarName, secondaryBaseJarName)
    }
}

task bundle(type: Zip) {
    dependsOn shadowJar, sparkJar, 'gatkTabComplete'

    doFirst {
        assert file("gatk-launch").exists()
        assert file("README.md").exists()
        assert file("build/docs/tabCompletion/gatk-launch-completion.sh").exists()
    }

    baseName = project.name + "-" + project.version
    destinationDir file("$buildDir")
    archiveName baseName + ".zip"

    from(shadowJar.archivePath)
    from(sparkJar.archivePath)
    from("gatk-launch")
    from("README.md")
    from("build/docs/tabCompletion/gatk-launch-completion.sh")

    into(baseName)

    doLast {
        logger.lifecycle("Created GATK distribution in ${destinationDir}/${archiveName}")
    }
}

task javadocJar(type: Jar, dependsOn: javadoc) {
    classifier = 'javadoc'
    from 'build/docs/javadoc'
}

task sourcesJar(type: Jar) {
    from sourceSets.main.allSource
    classifier = 'sources'
}

tasks.withType(Javadoc) {
    // do this for all javadoc tasks, including gatkDoc
    options.addStringOption('Xdoclint:none')
}

javadoc {
    // This is a hack to disable the java 8 default javadoc lint until we fix the html formatting
    // We only want to do this for the javadoc task, not gatkDoc
    options.addStringOption('Xdoclint:none', '-quiet')
}

// Generate GATK Online Doc
task gatkDoc(type: Javadoc, dependsOn: classes) {
    final File gatkDocDir = new File("build/docs/gatkdoc")
    doFirst {
        // make sure the output folder exists or we can create it
        if (!gatkDocDir.exists() && !gatkDocDir.mkdirs()) {
            throw new GradleException(String.format("Failure creating folder (%s) for GATK doc output in task (%s)",
                    gatkDocDir.getAbsolutePath(),
                    it.name));
        }
        copy {
            from('src/main/resources/org/broadinstitute/hellbender/utils/helpTemplates')
            include 'gatkDoc.css'
            into gatkDocDir
        }
    }
    source = sourceSets.main.allJava

    // The gatkDoc process instantiates any documented feature classes, so to run it we need the entire
    // runtime classpath, as well as jdk javadoc files such as tools.jar, where com.sun.javadoc lives.
    classpath = sourceSets.main.runtimeClasspath + javadocJDKFiles
    options.docletpath = classpath.asType(List)
    options.doclet = "org.broadinstitute.hellbender.utils.help.GATKHelpDoclet"

    outputs.dir(gatkDocDir)
    options.destinationDirectory(gatkDocDir)

    options.addStringOption("settings-dir", "src/main/resources/org/broadinstitute/hellbender/utils/helpTemplates");
    if (project.hasProperty('phpDoc')) {
        // use -PphpDoc to generate .php file extensions, otherwise rely on default of .html
        final String phpExtension = "php"
        options.addStringOption("output-file-extension", phpExtension)
        options.addStringOption("index-file-extension", phpExtension)
    }
    options.addStringOption("absolute-version", getVersion())
    options.addStringOption("build-timestamp", new Date().format("dd-mm-yyyy hh:mm:ss"))
    options.addStringOption("verbose")
}

// Generate GATK Bash Tab Completion File
task gatkTabComplete(type: Javadoc, dependsOn: classes) {
    final File tabCompletionDir = new File("build/docs/tabCompletion")
    doFirst {
        // make sure the output folder exists or we can create it
        if (!tabCompletionDir.exists() && !tabCompletionDir.mkdirs()) {
            throw new GradleException(String.format("Failure creating folder (%s) for GATK tab completion output in task (%s)",
                    tabCompletionDir.getAbsolutePath(),
                    it.name));
        }
    }
    source = sourceSets.main.allJava

    // The gatkDoc process instantiates any documented feature classes, so to run it we need the entire
    // runtime classpath, as well as jdk javadoc files such as tools.jar, where com.sun.javadoc lives.
    classpath = sourceSets.main.runtimeClasspath + javadocJDKFiles

    options.docletpath = classpath.asType(List)
    options.doclet = "org.broadinstitute.barclay.help.BashTabCompletionDoclet"

    outputs.dir(tabCompletionDir)
    options.destinationDirectory(tabCompletionDir)

    // This is a hack to work around a gross Gradle bug:
    options.addStringOption('use-default-templates', '-use-default-templates')

    options.addStringOption("output-file-extension", "sh")
    options.addStringOption("index-file-extension", "sh")
    options.addStringOption("absolute-version", getVersion())
    options.addStringOption("build-timestamp", new Date().format("dd-mm-yyyy hh:mm:ss"))

    options.addStringOption("caller-script-name", "gatk-launch")

    options.addStringOption("caller-pre-legal-args", "--help --list --dryRun --javaOptions")
    options.addStringOption("caller-pre-arg-val-types", "null null null String")
    options.addStringOption("caller-pre-mutex-args", "--help;list,dryRun,javaOptions --list;help,dryRun,javaOptions")
    options.addStringOption("caller-pre-alias-args", "--help;-h")
    options.addStringOption("caller-pre-arg-min-occurs", "0 0 0 0")
    options.addStringOption("caller-pre-arg-max-occurs", "1 1 1 1")

    options.addStringOption("caller-post-legal-args", "--sparkRunner --spark-master --cluster --dryRun --javaOptions --conf --driver-memory --driver-cores --executor-memory --executor-cores --num-executors")
    options.addStringOption("caller-post-arg-val-types", "String String String null String file int int int int int")
    options.addStringOption("caller-post-mutex-args", "")
    options.addStringOption("caller-post-alias-args", "")
    options.addStringOption("caller-post-arg-min-occurs", "0 0 0 0 0 0 0 0 0 0")
    options.addStringOption("caller-post-arg-max-occurs", "1 1 1 1 1 1 1 1 1 1")

    options.addStringOption("verbose", "-verbose")
}

/**
 *This specifies what artifacts will be built and uploaded when performing a maven upload.
 */
artifacts {
    archives jar
    archives javadocJar
    archives sourcesJar
}

/**
 * Sign non-snapshot releases with our secret key.  This should never need to be invoked directly.
 */
signing {
    required { isRelease && gradle.taskGraph.hasTask("uploadArchives") }
    sign configurations.archives
}

/**
 * Upload a release to sonatype.  You must be an authorized uploader and have your sonatype
 * username and password information in your gradle properties file.  See the readme for more info.
 *
 * For releasing to your local maven repo, use gradle install
 */
uploadArchives {
    doFirst {
        println "Attempting to upload version:$version"
    }
    repositories {
        mavenDeployer {
            beforeDeployment { MavenDeployment deployment -> signing.signPom(deployment) }

            repository(url: "https://oss.sonatype.org/service/local/staging/deploy/maven2/") {
                authentication(userName: project.findProperty("sonatypeUsername"), password: project.findProperty("sonatypePassword"))
            }

            snapshotRepository(url: "https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot-local/") {
                authentication(userName: System.env.ARTIFACTORY_USERNAME, password: System.env.ARTIFACTORY_PASSWORD)
            }

            pom.project {
                name 'GATK4'
                packaging 'jar'
                description 'Development on GATK 4'
                url 'http://github.com/broadinstitute/gatk'

                scm {
                    url 'scm:git@github.com:broadinstitute/gatk.git'
                    connection 'scm:git@github.com:broadinstitute/gatk.git'
                    developerConnection 'scm:git@github.com:broadinstitute/gatk.git'
                }

                developers {
                    developer {
                        id = "gatkdev"
                        name = "GATK Development Team"
                        email = "gatk-dev-public@broadinstitute.org"
                    }
                }

                licenses {
                    license {
                        name 'BSD 3-Clause'
                        url 'https://github.com/broadinstitute/gatk/blob/master/LICENSE.TXT'
                        distribution 'repo'
                    }
                }
            }
        }
    }
}

task installSpark{ dependsOn sparkJar }
task installAll{  dependsOn installSpark, installDist }

installDist.dependsOn downloadGsaLibFile

defaultTasks 'bundle'
