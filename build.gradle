//Note: this section 'buildscript` is only for the dependencies of the buildscript itself.
// See the second 'repositories' section below for the actual dependencies of GATK itself
buildscript {
    repositories {
        mavenCentral()
        jcenter() // for shadow plugin
     }
}

plugins {
    id "java"
    id "application"
    id 'maven'
    id 'signing'
    id "jacoco"
    id "de.undercouch.download" version "2.1.0" //used for downloading GSA lib
    id "com.github.johnrengelman.shadow" version "1.2.3"
    id "com.github.kt3k.coveralls" version "2.6.3"
    id "com.github.ben-manes.versions" version "0.12.0" //used for identifying dependencies that need updating
    id 'com.palantir.git-version' version '0.5.1' //version helper
}

import com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar
import de.undercouch.gradle.tasks.download.Download

mainClassName = "org.broadinstitute.hellbender.Main"

//Note: the test suite must use the same defaults. If you change system properties in this list you must also update the one in the test task
applicationDefaultJvmArgs = ["-Dsamjdk.use_async_io_read_samtools=false","-Dsamjdk.use_async_io_write_samtools=true", "-Dsamjdk.use_async_io_write_tribble=false", "-Dsamjdk.compression_level=1", "-Dsnappy.disable=true"]

//Delete the windows script - we never test on Windows so let's not pretend it works
startScripts {
    doLast {
        delete windowsScript
    }
}

task downloadGsaLibFile(type: Download) {
    src 'http://cran.r-project.org/src/contrib/gsalib_2.1.tar.gz'
    dest "src/main/resources/org/broadinstitute/hellbender/utils/R/gsalib.tar.gz"
    overwrite false
}


repositories {
    mavenCentral()
    jcenter()

    maven {
        url "https://artifactory.broadinstitute.org/artifactory/libs-snapshot/" //for htsjdk snapshots
    }

    mavenLocal()

}

final htsjdkVersion = System.getProperty('htsjdk.version','2.8.0')
final sparkVersion = System.getProperty('spark.version', '2.0.2')
final hadoopBamVersion = System.getProperty('hadoopBam.version','7.7.1')

configurations.all {
    resolutionStrategy {
        force 'com.google.http-client:google-http-client:1.21.0'
        // the snapshot folder contains a dev version of guava, we don't want to use that.
        force 'com.google.guava:guava:18.0'
        // force the htsjdk version so we don't get a different one transitively
        force 'com.github.samtools:htsjdk:' + htsjdkVersion
        // later versions explode Hadoop
        force 'com.google.protobuf:protobuf-java:3.0.0-beta-1'
    }
    all*.exclude group: 'org.slf4j', module: 'slf4j-jdk14' //exclude this to prevent slf4j complaining about to many slf4j bindings
    all*.exclude group: 'com.google.guava', module: 'guava-jdk5'
}

jacocoTestReport {
    dependsOn test
    group = "Reporting"
    description = "Generate Jacoco coverage reports after running tests."
    additionalSourceDirs = files(sourceSets.main.allJava.srcDirs)

    reports {
        xml.enabled = true // coveralls plugin depends on xml format report
        html.enabled = true
    }
}

//NOTE: we ignore contracts for now
compileJava {
  options.compilerArgs = ['-proc:none', '-Xlint:all','-Werror','-Xdiags:verbose']
}
compileTestJava {
  options.compilerArgs = ['-proc:none', '-Xlint:all','-Werror','-Xdiags:verbose']
}

dependencies {
    compile 'org.broadinstitute:barclay:1.0.0'
    compile 'com.intel:genomicsdb:0.3.0'
    compile 'com.opencsv:opencsv:3.4'
    compile 'com.google.guava:guava:18.0'
    compile 'com.github.samtools:htsjdk:'+ htsjdkVersion
    // Using the shaded version to avoid conflicts between its protobuf dependency
    // and that of Hadoop/Spark (either the one we reference explicitly, or the one
    // provided by dataproc).
    compile 'com.google.cloud:google-cloud-nio:0.5.1:shaded'
    compile 'com.google.cloud.genomics:google-genomics-dataflow:v1beta2-0.15'
    compile 'com.google.cloud.genomics:gatk-tools-java:1.1'
    compile 'org.apache.logging.log4j:log4j-api:2.3'
    compile 'org.apache.logging.log4j:log4j-core:2.3'
    compile 'org.apache.commons:commons-lang3:3.4'
    compile 'org.apache.commons:commons-math3:3.5'
    compile 'org.apache.commons:commons-collections4:4.1'
    compile 'org.apache.commons:commons-vfs2:2.0'
    compile 'commons-io:commons-io:2.4'
    compile 'org.reflections:reflections:0.9.10'
    compile 'net.sf.jopt-simple:jopt-simple:5.0-beta-1'
    compile 'com.google.cloud.dataflow:google-cloud-dataflow-java-sdk-all:0.4.150727'
    compile 'it.unimi.dsi:fastutil:7.0.6'
    compile 'com.google.apis:google-api-services-genomics:v1-rev90-1.22.0'
    compile 'com.google.cloud.genomics:google-genomics-utils:v1beta2-0.30'

    compile 'org.ojalgo:ojalgo:39.0'
    compile ('org.apache.spark:spark-mllib_2.11:' + sparkVersion) {
        // JUL is used by Google Dataflow as the backend logger, so exclude jul-to-slf4j to avoid a loop
        exclude module: 'jul-to-slf4j'
        exclude module: 'javax.servlet'
        exclude module: 'servlet-api'
    }

    compile 'org.bdgenomics.bdg-formats:bdg-formats:0.5.0'
    compile('org.bdgenomics.adam:adam-core-spark2_2.11:0.20.0') {
        exclude group: 'org.slf4j'
        exclude group: 'org.apache.hadoop'
        exclude group: 'org.scala-lang'
        exclude module: 'kryo'
        exclude module: 'hadoop-bam'
    }

    compile 'org.jgrapht:jgrapht-core:0.9.1'
    compile 'org.testng:testng:6.9.6' //compile instead of testCompile because it is needed for test infrastructure that needs to be packaged
    compile 'org.apache.hadoop:hadoop-minicluster:2.7.2' //the version of minicluster should match the version of hadoop

    compile('org.seqdoop:hadoop-bam:' + hadoopBamVersion) {
        exclude group: 'org.apache.hadoop'
        exclude module: 'htsjdk'
    }
    compile('org.apache.hadoop:hadoop-client:2.7.2') // should be a 'provided' dependency
    compile('com.github.jsr203hadoop:jsr203hadoop:1.0.3')

    compile('de.javakaffee:kryo-serializers:0.41') {
        exclude module: 'kryo' // use Spark's version
    }

    // Dependency change for including MLLib
    compile('org.objenesis:objenesis:1.2')
    testCompile('org.objenesis:objenesis:2.1')

    // Comment the next line to disable native code proxies in Spark MLLib
    compile('com.github.fommil.netlib:all:1.1.2')

    // Dependency change for including MLLib
    compile('com.esotericsoftware:kryo:3.0.3'){
        exclude group: 'com.esotericsoftware', module: 'reflectasm'
        exclude group: 'org.ow2.asm', module: 'asm'
    }

    // Dependency change for including MLLib
    compile('com.esotericsoftware:reflectasm:1.10.0:shaded')

    compile('com.intel.gkl:gkl:0.3.1') {
        exclude module: 'htsjdk'
    }
    
    //This is identical to jbwa:1.0.0 but includes an additional native library to support ppc64
    compile 'com.github.lindenb:jbwa:1.0.0_ppc64'

    //needed for DataflowAssert
    testCompile 'org.hamcrest:hamcrest-all:1.3'
    testCompile 'junit:junit:4.12'
    testCompile "org.mockito:mockito-core:1.10.19"
}

//add gatk-launch to the jar as a resource
processResources {
    from("gatk-launch")
}

processTestResources {
    //Don't waste time packaging our test data into the test jar
    exclude "**/large/**"
    exclude "**/org/broadinstitute/hellbender/**"
}

sourceCompatibility = 1.8
targetCompatibility = 1.8

final isRelease = Boolean.getBoolean("release")
version = (isRelease ? gitVersion() : gitVersion() + "-SNAPSHOT").replaceAll(".dirty", "")

logger.info("build for version:" + version)
group = 'org.broadinstitute'


tasks.withType(Jar) {
    manifest {
        attributes 'Implementation-Title': 'Hellbender-tools',
                'Implementation-Version': version,
                'Main-Class': project.mainClassName
    }
}

test {
    outputs.upToDateWhen { false }  //tests will never be "up to date" so you can always rerun them
    String CI = "$System.env.CI"
    String CLOUD = "$System.env.CLOUD"
    String SPARK = "$System.env.SPARK"
    useTestNG{
        if (CLOUD =="mandatory") {
            // run only the cloud tests
            includeGroups 'cloud', 'bucket'
        } else if (CLOUD == "todo") {
            // run only the in-development cloud tests
            includeGroups 'cloud_todo', 'bucket_todo'
        } else if (CLOUD == "together") {
            // run both local tests and mandatory cloud tests, together.
            // This is good when e.g. you are done for the day and want to run tests on your machine overnight.
            excludeGroups 'cloud_todo', 'bucket_todo'
        } else {
            // run only the local tests
            excludeGroups 'cloud', 'bucket', 'cloud_todo', 'bucket_todo'
        }
        if (SPARK == "false") {
            // exclude Spark tests
            doFirst {
                println( "Skipping Spark Tests")
            }
            excludeGroups 'spark'
        }
    }

    systemProperty "samjdk.use_async_io_read_samtools", "false"
    systemProperty "samjdk.use_async_io_write_samtools", "true"
    systemProperty "samjdk.use_async_io_write_tribble", "false"
    systemProperty "samjdk.compression_level", "1"
    systemProperty "snappy.disable", "true"
    systemProperty "gatk.spark.debug", System.getProperty("gatk.spark.debug")

    // set heap size for the test JVM(s)
    minHeapSize = "1G"
    maxHeapSize = "4G"


    if (CI == "true" && CLOUD == "false" ) {
        int count = 0
        // listen to events in the test execution lifecycle

        beforeTest { descriptor ->
            count++
            if( count % 10000 == 0) {
                logger.lifecycle("Finished "+ Integer.toString(count++) + " tests")
            }
        }
    } else {
        // show standard out and standard error of the test JVM(s) on the console
        testLogging.showStandardStreams = true
        beforeTest { descriptor ->
            logger.lifecycle("Running Test: " + descriptor)
        }

        // listen to standard out and standard error of the test JVM(s)
        onOutput { descriptor, event ->
            logger.lifecycle("Test: " + descriptor + " produced standard out/err: " + event.message )
        }
    }

    testLogging {
        testLogging {
            events "skipped", "failed"
            exceptionFormat = "full"
        }
        afterSuite { desc, result ->
            if (!desc.parent) { // will match the outermost suite
                println "Results: ${result.resultType} (${result.testCount} tests, ${result.successfulTestCount} successes, ${result.failedTestCount} failures, ${result.skippedTestCount} skipped)"
            }
        }
    }

}

task wrapper(type: Wrapper) {
    gradleVersion = '3.1'
}

tasks.withType(ShadowJar) {
    from(project.sourceSets.main.output)
    baseName = project.name + '-package'
    mergeServiceFiles()
    relocate 'com.google.common', 'org.broadinstitute.hellbender.relocated.com.google.common'
    zip64 true
    exclude 'log4j.properties' // from adam jar as it clashes with hellbender's log4j2.xml
}



// Dependency change for including MLLib
configurations {
    compile.exclude module: 'jul-to-slf4j'
    compile.exclude module: 'javax.servlet'
    compile.exclude module: 'servlet-api'
    compile.exclude group: 'com.esotericsoftware.kryo'

    sparkConfiguration {
        extendsFrom runtime
        // exclude Hadoop and Spark dependencies, since they are provided when running with Spark
        // (ref: http://unethicalblogger.com/2015/07/15/gradle-goodness-excluding-depends-from-shadow.html)
        exclude group: 'org.apache.hadoop'
        exclude module: 'spark-core_2.11'
        exclude group: 'org.slf4j'
        exclude module: 'jul-to-slf4j'
        exclude module: 'javax.servlet'
        exclude module: 'servlet-api'
        exclude group: 'com.esotericsoftware.kryo'
        exclude module: 'spark-mllib_2.11'
        exclude group: 'org.scala-lang'
        exclude module: 'kryo'
    }
}

shadowJar {
    configurations = [project.configurations.runtime]
    classifier = 'local'
    mergeServiceFiles('reference.conf')
}

task localJar{ dependsOn shadowJar }

task sparkJar(type: ShadowJar) {
    group = "Shadow"
    description = "Create a combined jar of project and runtime dependencies that excludes provided spark dependencies"
    configurations = [project.configurations.sparkConfiguration]
    classifier = 'spark'
}

task gatkZipDistribution(type: Zip) {
    dependsOn shadowJar, sparkJar

    doFirst {
        assert file("settings.gradle").exists()
        assert file("gatk-launch").exists()
        assert file("README.md").exists()
    }

    baseName = project.name + "-" + project.version
    destinationDir file("$buildDir")
    archiveName baseName + ".zip"

    from(shadowJar.archivePath)
    from(sparkJar.archivePath)
    from("settings.gradle")
    from("gatk-launch")
    from("README.md")
    into(baseName)
}

task javadocJar(type: Jar, dependsOn: javadoc) {
    classifier = 'javadoc'
    from 'build/docs/javadoc'
}

task sourcesJar(type: Jar) {
    from sourceSets.main.allSource
    classifier = 'sources'
}

// This is a hack to disable the java 8 default javadoc lint until we fix the html formatting
if (JavaVersion.current().isJava8Compatible()) {
    tasks.withType(Javadoc) {
        options.addStringOption('Xdoclint:none', '-quiet')
    }
}

/**
 *This specifies what artifacts will be built and uploaded when performing a maven upload.
 */
artifacts {
    archives jar
    archives javadocJar
    archives sourcesJar
}

/**
 * Sign non-snapshot releases with our secret key.  This should never need to be invoked directly.
 */
signing {
    required { isRelease && gradle.taskGraph.hasTask("uploadArchives") }
    sign configurations.archives
}

/**
 * Upload a release to sonatype.  You must be an authorized uploader and have your sonatype
 * username and password information in your gradle properties file.  See the readme for more info.
 *
 * For releasing to your local maven repo, use gradle install
 */
uploadArchives {
    doFirst {
        println "Attempting to upload version:$version"
    }
    repositories {
        mavenDeployer {
            beforeDeployment { MavenDeployment deployment -> signing.signPom(deployment) }

            repository(url: "https://oss.sonatype.org/service/local/staging/deploy/maven2/") {
                authentication(userName: project.findProperty("sonatypeUsername"), password: project.findProperty("sonatypePassword"))
            }

            snapshotRepository(url: "https://artifactory.broadinstitute.org/artifactory/libs-snapshot-local/") {
                authentication(userName: System.env.ARTIFACTORY_USERNAME, password: System.env.ARTIFACTORY_PASSWORD)
            }

            pom.project {
                name 'GATK4'
                packaging 'jar'
                description 'Development on GATK 4'
                url 'http://github.com/broadinstitute/gatk'

                scm {
                    url 'scm:git@github.com:broadinstitute/gatk.git'
                    connection 'scm:git@github.com:broadinstitute/gatk.git'
                    developerConnection 'scm:git@github.com:broadinstitute/gatk.git'
                }

                developers {
                    developer {
                        id = "gatkdev"
                        name = "GATK Development Team"
                        email = "gatk-dev-public@broadinstitute.org"
                    }
                }

                licenses {
                    license {
                        name 'BSD 3-Clause'
                        url 'https://github.com/broadinstitute/gatk/blob/master/LICENSE.TXT'
                        distribution 'repo'
                    }
                }
            }
        }
    }
}



task installSpark{ dependsOn sparkJar }
task installAll{  dependsOn installSpark, installDist }

installDist.dependsOn downloadGsaLibFile
