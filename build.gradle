//Note: this section 'buildscript` is only for the dependencies of the buildscript itself.
// See the second 'repositories' section below for the actual dependencies of GATK itself
buildscript {
    repositories {
        mavenCentral()
    }
}

plugins {
    id "java"  // set up default java compile and test tasks
    id "application"  // provides installDist
    id "jacoco"  // record code coverage during test execution
    id "com.github.johnrengelman.shadow" version "1.2.3"  //used to build the shadow and sparkJars
    id "com.github.kt3k.coveralls" version "2.6.3"  // report coverage data to coveralls
    id "com.github.ben-manes.versions" version "0.12.0" //used for identifying dependencies that need updating
}

mainClassName = "org.broadinstitute.hellbender.Main"

//Note: the test suite must use the same defaults. If you change system properties in this list you must also update the one in the test task
//Note: this is copied verbatim from gatk public
applicationDefaultJvmArgs = ["-Dsamjdk.use_async_io_read_samtools=false","-Dsamjdk.use_async_io_write_samtools=true", "-Dsamjdk.use_async_io_write_tribble=false", "-Dsamjdk.compression_level=1"]

import com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar
import org.gradle.internal.os.OperatingSystem

def isMacOsX = OperatingSystem.current().macOsX
def customJarPath = hasProperty("custom.jar.dir") ? property("custom.jar.dir") : null;

repositories {
    mavenCentral()
    maven {
      url "https://oss.sonatype.org/content/groups/public"
    }
    maven {
        url "https://repository.cloudera.com/artifactory/cloudera-repos/" // spark-dataflow
    }
    maven {
        url "https://artifactory.broadinstitute.org/artifactory/libs-snapshot/" //for htsjdk snapshots
    }

    if (customJarPath) {
        flatDir {
            // Specified by user
            dirs customJarPath
        }
    }
    mavenLocal()
}

jacocoTestReport {
    dependsOn test
    group = "Reporting"
    description = "Generate Jacoco coverage reports after running tests."
    additionalSourceDirs = files(sourceSets.main.allJava.srcDirs)

    reports {
        xml.enabled = true // coveralls plugin depends on xml format report
        html.enabled = true
    }
}

jacoco {
    toolVersion = "0.7.1.201405082137"
}

//NOTE: we ignore contracts for now
compileJava {
   options.compilerArgs = ['-proc:none', '-Xlint:all', '-Werror']
}

compileTestJava {
   options.compilerArgs = ['-proc:none', '-Xlint:all', '-Werror']
}

build.dependsOn installDist
check.dependsOn installDist

dependencies {
    compile 'org.broadinstitute:gatk:4.alpha.2-130-g6a23efd-20170103.175407-1'
    compile 'org.broadinstitute:hdf5-java-bindings:1.1.0-hdf5_2.11.0'

    testCompile 'org.testng:testng:6.8.8'
}

// Dependency change for including MLLib
configurations {
    compile.exclude module: 'jul-to-slf4j'
    compile.exclude module: 'javax.servlet'
    compile.exclude module: 'servlet-api'
    compile.exclude group: 'com.esotericsoftware.kryo'

    sparkConfiguration {
        extendsFrom runtime
        // exclude Hadoop and Spark dependencies, since they are provided when running with Spark
        // (ref: http://unethicalblogger.com/2015/07/15/gradle-goodness-excluding-depends-from-shadow.html)
        exclude group: 'org.apache.hadoop'
        exclude module: 'spark-core_2.11'
        exclude group: 'org.slf4j'
        exclude module: 'jul-to-slf4j'
        exclude module: 'javax.servlet'
        exclude module: 'servlet-api'
        exclude group: 'com.esotericsoftware.kryo'
        exclude module: 'spark-mllib_2.11'
        exclude group: 'org.scala-lang'
        exclude module: 'kryo'
    }
}

def findJarByName(Configuration config, String name) {
    config.find { File file -> file.name.startsWith(name)}
}

final GATK_LAUNCH = "gatk-launch"
task setupGatkLaunch(type: Copy) {
    def gatkJar =  findJarByName(configurations.compile, 'gatk')

    from {
        zipTree( gatkJar ).matching{ include GATK_LAUNCH }.singleFile
    }
    into "$projectDir"
    outputs.file "$projectDir/$GATK_LAUNCH"
    description = "extract gatk-launch script from the gatk jar"
}

clean{
    delete GATK_LAUNCH //add gatk-launch script to things that need cleaning up since it isn't included automatically
}

sourceCompatibility = 1.8
targetCompatibility = 1.8

def String deriveVersion(){
    def stdout = new ByteArrayOutputStream()
    try {
        logger.info("path is $System.env.PATH")
        exec {
            commandLine "git", "describe", "--always"
            standardOutput = stdout;

            ignoreExitValue = true
        }
    } catch (GradleException e) {
        logger.error("Couldn't determine version.  " + e.getMessage())
    }
    return stdout.size() > 0 ? stdout.toString().trim() : "version-unknown"
}

def createSymlinks(archivePath, symlinkLocation) {
    exec {
        commandLine 'ln', '-fs', archivePath, symlinkLocation
        ignoreExitValue = false
    }
}

// Suffix is what will be added to the symlink
def createAllSymlinks(destinationDir, archivePath, suffix) {
    def finalSuffix = "-" + suffix
    if (suffix == "") {
        finalSuffix = ""
    }

    def symlinkLocation = destinationDir.toString() + "/hellbender-protected" + finalSuffix + ".jar"
    def symlinkLocation2 = destinationDir.toString() + "/gatk-protected" + finalSuffix + ".jar"

    createSymlinks(archivePath.getAbsolutePath(), symlinkLocation)
    createSymlinks(archivePath.getAbsolutePath(), symlinkLocation2)
}

version = deriveVersion()
final SNAPSHOT = "-SNAPSHOT"
version = deriveVersion() + SNAPSHOT
boolean isRelease = ! version.endsWith(SNAPSHOT)
logger.info("build for version:" + version);
group = 'org.broadinstitute'


tasks.withType(Jar) {
    manifest {
        attributes 'Implementation-Title': 'Hellbender-Protected-Tools',
                'Implementation-Version': version,
                'Main-Class': 'org.broadinstitute.hellbender.Main'
    }
}

test {

    systemProperty "samjdk.use_async_io_read_samtools", "false"
    systemProperty "samjdk.use_async_io_write_samtools", "true"
    systemProperty "samjdk.use_async_io_write_tribble", "false"
    systemProperty "samjdk.compression_level", "1"
    systemProperty "gatk.spark.debug", System.getProperty("gatk.spark.debug")

    environment "SPARK_LOCAL_IP","127.0.0.1"
    // enable TestNG support (default is JUnit)
    useTestNG{
        excludeGroups 'cloud', 'bucket'
    }

    // set heap size for the test JVM(s)
    jvmArgs = ['-Xmx6G']

    String CI = "$System.env.CI"

    maxParallelForks = CI == "true" ? 1 : 2

    if (CI == "true") {
        int count = 0
        // listen to events in the test execution lifecycle
        testLogging {
            events "skipped", "failed"
            exceptionFormat = "full"
        }

        beforeTest { descriptor ->
            count++
            if( count % 100 == 0) {
                logger.lifecycle("Finished " + Integer.toString(count++) + " tests")
            }
        }
    } else {
        // show standard out and standard error of the test JVM(s) on the console
        testLogging.showStandardStreams = true
        beforeTest { descriptor ->
            logger.lifecycle("Running Test: " + descriptor)
        }

        // listen to standard out and standard error of the test JVM(s)
        onOutput { descriptor, event ->
            logger.lifecycle("Test: " + descriptor + " produced standard out/err: " + event.message )
        }
    }
}

task wrapper(type: Wrapper) {
    gradleVersion = '3.1'
}

tasks.withType(ShadowJar) {
    from(project.sourceSets.main.output)
    baseName = project.name + '-all'
    mergeServiceFiles()

    // Suggested by the akka devs to make sure that we do not get the spark configuration error.
    // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler
    transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {
        resource = 'reference.conf'
    }


    relocate 'com.google.common', 'org.broadinstitute.hellbender.relocated.com.google.common'
    zip64 true
    exclude 'log4j.properties' // from adam jar as it clashes with hellbender's log4j2.xml
}

shadowJar {
    classifier = 'spark_standalone'
    doLast {
        // Create a symlink to the newly created jar.  The name will be hellbender-protected.jar and
        //  it will be at the same level as the newly created jar.  (overwriting symlink, if it exists)
        // Please note that this will cause failures in Windows, which does not support symlinks.
        createAllSymlinks(destinationDir.toString(), archivePath, "")
    }
}

task sparkJar(type: ShadowJar) {
    configurations = [project.configurations.sparkConfiguration]
    classifier = 'spark'
    doLast {
        // Create a symlink to the newly created jar.  The name will be hellbender-protected.jar and
        //  it will be at the same level as the newly created jar.  (overwriting symlink, if it exists)
        // Please note that this will cause failures in Windows, which does not support symlinks.
        createAllSymlinks(destinationDir.toString(), archivePath, classifier)
    }
}


task installSpark{ dependsOn sparkJar }
task installAll{  dependsOn installSpark, installDist }

tasks.withType(Jar) { dependsOn setupGatkLaunch }
