# AoU Template - Joint Calling with the GVS for AoU

**Note** The markdown source for this quickstart is maintained in the the  [GATK GitHub Repository](https://github.com/broadinstitute/gatk/blob/ah_var_store/scripts/variantstore/TERRA_QUICKSTART.md).  Submit any feedback, corrections or improvements in a pull request there.  Do not edit this file directly.

## Overview
Through this QuickStart you will learn how to use the Broad Genomic Variant Store to create a GATK VQSR Filtered joint callset VCF for whole genome samples.

The sequencing data in this quickstart came from the [AnVIL 1000G High Coverage workspace](https://app.terra.bio/#workspaces/anvil-datastorage/1000G-high-coverage-2019)

**Note:** VQSR dies with the default/recommended configuration, so we set SNP max-gaussians to 4 here.

## Prerequisites

This quickstart assumes that you are familiar with Terra workspaces, the data model and providing input parameters and launching workflows.

1. You will need to have or create a BigQuery dataset (we'll call this `datasetname` later on).
2. Grant the "BigQuery Editor" role on that **dataset** to your Terra PROXY group.  Your proxy group name can be found on your Terra Profile page and look something like `PROXY_3298237498237948372@firecloud.org`
3. Grant the following roles on the Google **project** containing the dataset to your proxy group
    - BigQuery data editor
    - BigQuery job user
    - BigQuery Read Session User
4. These tools expect re-blocked gVCF files as input.

## 1. Setup workspace with data model
This workspace has two files you can use to initially populate the data model. Both files are in the `Files` section of the workspace. One named `samples_10.tsv` has 10 samples with already reblocked gvcfs. The other file `samples_3202.tsv` has 3202 samples only a few of which have their reblocked gvcf attributes filled in.

## 2. Reblock samples
Run the GvsAoUReblockGvcf workflow on the samples

These are the required parameters which must be supplied to the workflow:

| Parameter         | Description |
|-------------------| ----------- |
| gvcf              | The gvcf file to reblock |
| ref_dict          | The reference dictionary to use       |
| ref_fasta         | The reference to use      |
| ref\_fasta_index  | The reference index       |


**Note:**
GVS defaults to expect reblocking v2 such that GQ40 is the highest quality band
If the data is reblocked with v1 and GQ50 and GQ60 are both present in the gVCFs, this will need to be set in the GvsImportSample step and the GvsExtractCallset step
drop_state="SIXTY" (the default is "FORTY")

## 3. Load data
### 3.1 Assign Gvs IDs (suggest renaming to GvsPrepareSamples)
To optimize the internal queries, each sample must have a unique and consecutive integer ID assigned. Run the `GvsAssignIds` workflow on a sample set, which will create an unique gvs id, update the BQ sample\_info table (creating it if it doesn't exists), and update the data model. This workflow takes care of creating the BQ vet and ref_ranges tables needed for the sample ids generated.

These are the required parameters which must be supplied to the workflow:

| Parameter              | Description |
|------------------------| ----------- |
| project_id             | The name of the google project containing the dataset |
| dataset_name           | The name of the dataset you created above       |
| external\_sample_names | Sample ids from the data model that will be mapped to gvs ids       |

If you will be using control samples in with your data and would like to assign those values manually you can pick an optional number to start the ids at so that there are reserved lower ordinal ids to use later
This optional parameter is: maxId


### 3.2 Import Samples

Next, your re-blocked gVCF files should be imported into GVS by running the `GvsImportSample` workflow which now works on individual samples. The workflow will check whether data for that sample has already been loaded into GVS.

These are the required parameters which must be supplied to the workflow:

| Parameter             | Description |
|-----------------------| ----------- |
| project_id            | The name of the google project containing the dataset |
| dataset_name          | The name of the dataset you created above       |
| external\_sample_name | The name of the sample from the data model |
| gvs\_sample_id        | The gvs_id from the data model |
| interval_list         | The interval list to use |

**Note:**
Samples that will be batched and loaded together must be put into a sample_set ahead of time, otherwise their loading may cause conflicts.
- This workflow must be done piecemeal if over 4000 samples are to be loaded as only 4000 samples can be loaded in at a time. The best way to do this currently is to create sample_sets of 4000 samples each.
- The workflow can then be run once for each sample_set. If the same sample_set is inadvertently run twice the workflow will detect that the samples already exist in the system and the workflow will fail.
- If any of the imports have failed on a single sample, check that all of the other samples have been loaded in that sample_set during that workflow. Sometimes a sample will fail during loading while there are still samples in the queue waiting for loading to begin. Because of the failure, these samples will not be loaded at all.
- Keep track of the samples that have not been loaded whether because they failed, or because they were in the queue when another sample failed. They will need to be added later.

Once all sample_sets have been run, if there have been any failures, collect all non-loaded samples together in a new sample_set and load that in.

Check the status table to see what samples have a STARTED log, but do not have a FINISHED log.
>SELECT info.sample_name, info.sample_id from `<PROJECT>.<DATASET>.sample_info` as info left join `<PROJECT>.<DATASET>.sample_load_status` as status on info.sample_id=status.sample_id where status.sample_id is null order by info.sample_id

A sample cannot be partially loaded into a single table, however it is possible for a sample to fail before it has been loaded into all the necessary tables. A sample may be loaded into either a vet or ref_ranges table, but not the other.
For the samples that failed during import, it's important to check if they were loaded into either the vet and ref_ranges table. They may have been loaded into both--in which case, yay! That sample is all set.
They may have been loaded into neither, in which case they will need to be loaded again with the other non-loaded samples. If they are in the vet table, but not the ref_ranges table or the reverse, you will need to remove them from that table with the following query:
`DELETE FROM <DATASET>.ref_ranges_015 WHERE sample_id = 56238`
`DELETE FROM <DATASET>.sample_load_status WHERE sample_id IN (56238)`


### 3.3 Update the is_loaded field
This is currently a manual step.
Run

> update `<PROJECT>.<DATASET>.sample_info` set is_loaded = TRUE where cast(sample_id as STRING) in (
select partition_id from `<PROJECT>.<DATASET>.INFORMATION_SCHEMA.PARTITIONS` where total_logical_bytes > 0 AND table_name like 'vet_%'
)


## 4. Create Alt Allele Table
This step loads data into the ALT_ALLELE table from the `vet_*` tables.

This is done by running the `GvsCreateAltAllele` workflow with the following parameters:

| Parameter      | Description |
| ----------------- | ----------- |
| data_project | The name of the google project containing the dataset |
| default_dataset      | The name of the dataset  |

**Note:** This workflow does not use the Terra Entity model to run, so be sure to select `Run workflow with inputs defined by file paths`

## 5. Create Filter Set

This step calculates features from the ALT_ALLELE table, and trains the VQSR filtering model along with site-level QC filters and loads them into BigQuery into a series of `filter_set_*` tables.

This is done by running the `GvsCreateFilterSet` workflow with the following parameters:

| Parameter      | Description |
| ----------------- | ----------- |
| data_project | The name of the google project containing the dataset |
| default_dataset      | The name of the dataset  |
| filter\_set_name | A unique name to identify this filter set (e.g. `my_demo_filters` ); you will want to make note of this for use in step 4  |

**Note:** This workflow does not use the Terra Entity model to run, so be sure to select `Run workflow with inputs defined by file paths`

Sometimes this workflow will fail because the Gaussians have not converged. Dont panic! It can happen to anyone's data!
The first step in this case will be to adjust the Guassian for the failed step (there are two possible steps: model creation for the SNPS and model creation for the InDels) to a lower number. 
By default, in the WDL, the max number of guassians attempted is 4 for InDels and 6 for SNPs.
- IndelsVariantRecalibrator.max_gaussians
- SNPsVariantRecalibratorClassic.max_gaussians (if you’re creating a callset with less than 20,000 samples)
- SNPsVariantRecalibratorCreateModel.max_gaussians (if you’re creating a callset with more than 20,000 samples)
You can then kick off the workflow again. 

When running the workflow additional times, make sure to change the filter_set_name (and for consistency the output_file_basename) so that the results do not overwrite the previous run

## 6. Extract Cohort

This step extracts the data in BigQuery and transforms it into a sharded joint called VCF incorporating the VQSR filter set

This is done by running the `GvsExtractCallset` workflow with the following parameters:

TODO:
 - remove "extract_table_prefix" from WDL requirements (why can't we do this yet?)
 - optimize the samples_to_extract information
 - we can make "mode" of ranges the default (or at least stick it in the methods config)

| Parameter                      | Description |
|--------------------------------| ----------- |
| data_project                   | The name of the google project containing the dataset |
| default_dataset                | The name of the dataset  |
| filter\_set_name               | the name of the filter set identifier created in step #3 |
| extract\_table_prefix          | set to any value |
| mode                           | "RANGES" |
| fq\_samples\_to\_extract_table | <data_project>.<default_dataset>.sample_info |
| output_file\_base\_name        | Base name for generated VCFs |

**Note:** This workflow does not use the Terra Entity model to run, so be sure to select `Run workflow with inputs defined by file paths`

## 5. Your VCF is ready!!

The sharded VCF outut files are listed in the `ExtractTask.output_vcf` workflow output, and the associated index files are listed in `ExtractTask.output_vcf_index`