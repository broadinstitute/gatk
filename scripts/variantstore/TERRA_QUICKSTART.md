# Quickstart - Joint Calling with the Broad Genomic Variant Store 

**Note:** The markdown source for this quickstart is maintained in the the  [GATK GitHub Repository](https://github.com/broadinstitute/gatk/blob/ah_var_store/scripts/variantstore/TERRA_QUICKSTART.md). Submit any feedback, corrections or improvements in a pull request there.  Do not edit this file directly.

## Overview
Through this QuickStart you will learn how to use the Broad Genomic Variant Store to create a GATK VQSR Filtered joint callset VCF for 10 whole genome samples. The sequencing data in this quickstart came from the [AnVIL 1000G High Coverage workspace](https://app.terra.bio/#workspaces/anvil-datastorage/1000G-high-coverage-2019).


## Prerequisites
This quickstart assumes that you are familiar with Terra workspaces, the data model and providing input parameters and launching workflows.

1. You will need to have or create a BigQuery dataset (we'll call this `dataset_name` later on).
2. Grant the following roles on the Google **project** (we'll call this `project_id` later on) containing the dataset to your proxy group:
    - BigQuery data editor
    - BigQuery job user
    - BigQuery Read Session User
3. These tools expect re-blocked gVCF files as input, which are provided in this workspace

## 1. Import Data
A sample set for the quickstart has already been created with 10 samples and paths to re-blocked gVCFs for each sample.  Import the sample data from this sample set by running the `GvsBulkIngestGenomes` workflow.

This workflow does not use the Terra data model to run, so be sure to select `Run workflow with inputs defined by file paths`.

These are the required parameters which must be supplied to the workflow:

| Parameter             | Description                                                                   |
| --------------------- |-------------------------------------------------------------------------------|
| dataset_name          | the name of the dataset you created above                                     |
| project_id            | the name of the google project containing the dataset                         |

If you are creating your own sample set, note that the sample table should have columns for the re-blocked gVCFs (`hg38_reblocked_v2_vcf` or `reblocked_gvcf_path`) and their index files, and the `sample_set_name` parameter described below should be specified:

| Parameter       | Description                               |
|-----------------|-------------------------------------------|
| sample_set_name | the name of the sample set to be ingested |

## 1. Create Alt Allele Table
This step loads data into the ALT_ALLELE table from the `vet_*` tables.

This workflow does not use the Terra data model to run, so be sure to select `Run workflow with inputs defined by file paths`.

This is done by running the `GvsPopulateAltAllele` workflow with the following parameters:

| Parameter         | Description |
| ----------------- | ----------- |
| call_set_identifier | a unique name to identify this callset (e.g. `my_gvs_demo`); you will want to make note of this for later steps |
| dataset_name      | the name of the dataset you created above  |
| project_id        | the name of the google project containing the dataset |

## 1. Create Filter Set
This step calculates features from the ALT_ALLELE table, trains the VQSR filtering model along with site-level QC filters, and loads them into BigQuery into a series of `filter_set_*` tables.  

This workflow does not use the Terra data model to run, so be sure to select `Run workflow with inputs defined by file paths`.

This is done by running the `GvsCreateFilterSet` workflow with the following parameters:

| Parameter                         | Description |
| --------------------------------- | ----------- |
| dataset_name                      | the name of the dataset you created above  |
| filter_set_name                   | a unique name to identify this filter set (e.g. `my_demo_filters`); you will want to make note of this for use in step 5 |
| project_id                        | the name of the google project containing the dataset |

## 1. Prepare Callset
This step performs the heavy lifting in BigQuery to gather all the data required to create a jointly called VCF.

This is done by running the `GvsPrepareRangesCallset` workflow with the following parameters:

| Parameter            | Description |
|--------------------- | ----------- |
| call_set_identifier      | a unique, descriptive name for the callset, this should be the same as the `call_set_identifier` from step 2  |
| dataset_name         | the name of the dataset you created above  |
| extract_table_prefix | A unique, descriptive name for the tables containing the callset (for simplicity, you can use the same name you used for `filter_set_name` in step 3); you will want to make note of this for use in the next step |
| project_id           | the name of the google project containing the dataset |

## 1. Extract Cohort
Now the data is ready to be extracted!

This workflow does not use the Terra data model to run, so be sure to select `Run workflow with inputs defined by file paths`.

This is done by running the `GvsExtractCallset` workflow with the following parameters:

| Parameter            | Description              |
| -------------------- | -------------------------|
| dataset_name         | the name of the dataset you created above  |
| extract_table_prefix | the unique, descriptive name for the tables containing the callset you chose in step 4  |
| filter_set_name      | the name of the filter set created in step 3  |
| project_id           | the name of the google project containing the dataset |

## 1. Your VCF files are ready!

The sharded VCF output files are listed in the `ExtractTask.output_vcf` workflow output, and the associated index files are listed in `ExtractTask.output_vcf_index`.
