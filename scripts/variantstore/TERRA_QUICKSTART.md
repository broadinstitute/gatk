# Quickstart - Joint Calling with the Broad Genomic Variant Store 

**Note** The markdown source for this quickstart is maintained in the the  [GATK GitHub Repository](https://github.com/broadinstitute/gatk/blob/ah_var_store/scripts/variantstore/TERRA_QUICKSTART.md).  Submit any feedback, corrections or improvements in a pull request there.  Do not edit this file directly.

## Overview
Through this QuickStart you will learn how to use the Broad Genomic Variant Store to create a GATK VQSR Filtered joint callset VCF for whole genome samples.

The sequencing data in this quickstart came from the [AnVIL 1000G High Coverage workspace](https://app.terra.bio/#workspaces/anvil-datastorage/1000G-high-coverage-2019)

**Note:** VQSR dies with the default/recommended configuration, so we set SNP max-gaussians to 4 here.

## Prerequisites

This quickstart assumes that you are familiar with Terra workspaces, the data model and providing input parameters and launching workflows.

1. You will need to have or create a BigQuery dataset (we'll call this `datasetname` later on). 
2. Grant the "BigQuery Editor" role on that **dataset** to your Terra PROXY group.  Your proxy group name can be found on your Terra Profile page and look something like `PROXY_3298237498237948372@firecloud.org`
3. Grant the following roles on the Google **project** containing the dataset to your proxy group
    - BigQuery data editor
    - BigQuery job user
    - BigQuery Read Session User
4. These tools expect re-blocked gVCF files as input.

## 1. Setup workspace with data model
The quickstart workspace has a file named sample.tsv that contains the attributes for 26 samples. Load this file into your data table.  This will set up the `sample_id`, `gvcf`, and `research_id` attributes. 

## 2. Reblock samples
Run the GvsAoUReblockGvcf workflow on the samples

These are the required parameters which must be supplied to the workflow:

| Parameter      | Description |
| ----------------- | ----------- |
| gvcf | The gvcf file to reblock |
| ref_dict      		| The reference dictionary to use       |
| ref_fasta      	| The reference to use      |
| ref_fasta_index   | The reference index       |


## 3. Import samples
### 3.1 Assign Gvs IDs
To optimize the internal queries, each sample must have a unique and consecutive integer ID assigned. Run the `GvsAssignIds` workflow, which will create an appropriate ID for each sample in the sample set, update the BigQuery dataset with the sample name to ID mapping info and update the data model with the gvs id assigned.

These are the required parameters which must be supplied to the workflow:

| Parameter      | Description |
| ----------------- | ----------- |
| project_id | The name of the google project containing the dataset |
| dataset_name      | The name of the dataset you created above       |
| external\_sample_names      | Sample ids from the data model that will be mapped to gvs ids       |
| workspace_namespace      | The namespace of the workspace       |
| workspace_name      | The name of the workspace       |


### 3.2 Create Tables
TODO: Automate this
Run this workflow if you are ingesting samples into a table id that hasn't been created yet. 

### 3.3 Load data

Next, your re-blocked gVCF files should be imported into GVS by running the `GvsImportGenomes` workflow.

TODO: 
 - we can change the WDLs to make the pet/ref_ranges flags the default
 
These are the required parameters which must be supplied to the workflow:

| Parameter      | Description |
| ----------------- | ----------- |
| project_id | The name of the google project containing the dataset |
| dataset_name      | The name of the dataset you created above       |
| load_pet      | false       |
| load\_ref_ranges      | true       |
| output_directory | A unique GCS path to be used for loading, can be in the workspace bucket.  E.g. `gs://fc-124-12-132-123-31/gvs/demo1`)


**NOTE**: if your workflow fails, you will need to manually remove a lockfile from the output directory.  It is called LOCKFILE, and can be removed with `gsutil rm`

## 4. Create Alt Allele Table
This step loads data into the ALT_ALLELE table from the `vet_*` tables.

This is done by running the `GvsCreateAltAllele` workflow with the following parameters:

| Parameter      | Description |
| ----------------- | ----------- |
| data_project | The name of the google project containing the dataset |
| default_dataset      | The name of the dataset  |

**Note:** This workflow does not use the Terra Entity model to run, so be sure to select `Run workflow with inputs defined by file paths`

## 5. Create Filter Set

This step calculates features from the ALT_ALLELE table, and trains the VQSR filtering model along with site-level QC filters and loads them into BigQuery into a series of `filter_set_*` tables.  

This is done by running the `GvsCreateFilterSet` workflow with the following parameters:

| Parameter      | Description |
| ----------------- | ----------- |
| data_project | The name of the google project containing the dataset |
| default_dataset      | The name of the dataset  |
| filter\_set_name | A unique name to identify this filter set (e.g. `my_demo_filters` ); you will want to make note of this for use in step 4  |

**Note:** This workflow does not use the Terra Entity model to run, so be sure to select `Run workflow with inputs defined by file paths`

## 6. Extract Cohort

This step extracts the data in BigQuery and transforms it into a sharded joint called VCF incorporating the VQSR filter set

This is done by running the `GvsExtractCallset` workflow with the following parameters:

TODO: 
 - remove "extract_table_prefix" from WDL requirements (why can't we do this yet?)
 - optimize the samples_to_extract information
 - we can make "mode" of ranges the default (or at least stick it in the methods config)
 
| Parameter      | Description |
| ----------------- | ----------- |
| data_project | The name of the google project containing the dataset |
| default_dataset      | The name of the dataset  |
| filter\_set_name | the name of the filter set identifier created in step #3 |
| extract\_table_prefix | set to any value |
| mode | "RANGES" |
| fq\_samples\_to\_extract_table | <data_project>.<default_dataset>.sample_info |
| output_file\_base\_name | Base name for generated VCFs |

**Note:** This workflow does not use the Terra Entity model to run, so be sure to select `Run workflow with inputs defined by file paths`

## 5. Your VCF is ready!!

The sharded VCF outut files are listed in the `ExtractTask.output_vcf` workflow output, and the associated index files are listed in `ExtractTask.output_vcf_index`

