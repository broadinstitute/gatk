# Getting started with GvsJointVariantCalling.wdl - Joint Calling with the Broad Genomic Variant Store 

**Note:** The markdown source for this quickstart is maintained in the the  [GATK GitHub Repository](https://github.com/broadinstitute/gatk/blob/ah_var_store/scripts/variantstore/wdl/README.md). Submit any feedback, corrections or improvements in a pull request there.  Do not edit this file directly.



## Overview
With this pipeline you will use the Broad Genomic Variant Store to create a GATK VQSR Filtered joint callset VCF.

The GvsUnified WDL is a wrapper WDL that expedites the creation of a callset using the GVS pipeline.
It is a wrapper for the common GVS pipeline.
It sets many params to common defaults. 
It is meant as a simple way of running a complex process, so it is straightforward to run and inflexible.

## Prerequisites
This quickstart assumes that you are familiar with Terra workspaces, the data model and providing input parameters and launching workflows.

Who creates the BQ project??? Is that us?

1. You will need to have or create a BigQuery dataset (this corresponds to the input param: `dataset_name`).
2. Grant the "BigQuery Data Editor" role on that **dataset** to your Terra PROXY group.  Your proxy group name can be found on your Terra Profile page and look something like `PROXY_12345678901234567890@firecloud.org`.
3. Grant the following roles on the Google **project** (this corresponds to the input param: `project_id`) containing the dataset to your proxy group:
    - BigQuery data editor
    - BigQuery job user
    - BigQuery Read Session User
4. These tools expect re-blocked gVCF files as input. Instuctions on how to re-block gVCFs are below.
5. Cloning the Terra workspace [Genomic\_Variant\_Store\_beta\_dev\_version](https://app.terra.bio/#workspaces/help-terra/Genomic%20Variant%20Store%20beta%20dev%20version)


## TODO should there be a bit on loading your data into the Terra workspace?!??! And then running the python notebook to validate the data!??!?!
Link to documentation for Terra data loading and sharing the workspace
Link to docs on how to run a workflow (we'll import the unifed workflow already)
Dont run a python notebook! 
Great, now that data is in your workspace, email Kylee to have someone validate your data
and share the workspace

## Run the pipeline
Now that the samples have been validated and made into a sample set (is that still required?!?!?) run the GvsUnified.wdl against this sample set.
Do this by selecting "sample_set" as the root entity type ("Step 1" on the workflow submission page) and `gvs_sample_set` for the data ("Step 2" on the workflow submission page).  
Since you are creating your own sample set, note that the sample table should have columns for the re-blocked gVCFs (`hg38_reblocked_gvcf` or `reblocked_gvcf_path`) and their index files.


These are the required parameters which must be supplied to the workflow:

| Parameter             | Description                                                                               |
|-----------------------|-------------------------------------------------------------------------------------------|
| dataset_name          | the name of the dataset you created above                                                 |
| project_id            | the name of the google project containing the dataset                                     |
| external_sample_names | `this.samples.sample_id` (the sample identifier column from the `gvs_sample_set` sample set) |
| input_vcfs            | the name of the google project containing the dataset                                     |
| input_vcf_indexes     | the index files for the above gVCFs                                                       |
| filter_set_name       | the desired name for the exported callset (right? it's not just the filter externally?)   | <_ callset_identifier
| extract_table_prefix  | the desired name for the exported callset (right? it's not just the filter externally?)   |
| extract_scatter_count | (lets please not let the Beta users have to worry abt this)                               |




        # Optional params that I think we might want to just cut out for now?

        Boolean samples_are_controls = false (how would this even work in the unified wdl? if they are controls the filter is useless, right?)
        File? gatk_override (we still need this I suppose, but do we want users to see this? Do we need this in this wdl for testing?)
        String? service_account_json_path (def still need for AoU, but is this too confusing for Beta users? also soon AoU wont need either?)
        File interval_list = "gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.noCentromeres.noTelomeres.interval_list"
                         (^ are we planning on supporting long reads or exomes any time soon?)
        # The larger the `load_data_batch_size` the greater the probability of preemptions and non-retryable
        # BigQuery errors. So if increasing the batch size, then preemptible and maxretries should also be increased.
        Int load_data_batch_size = 5 (seems stressful for our Beta users to have to think about)
        Int? load_data_preemptible_override
        Int? load_data_maxretries_override
        Array[String] indel_recalibration_annotation_values = ["AS_FS", "AS_ReadPosRankSum", "AS_MQRankSum", "AS_QD", "AS_SOR"]
        Array[String] snp_recalibration_annotation_values = ["AS_QD", "AS_MQRankSum", "AS_ReadPosRankSum", "AS_FS", "AS_MQ", "AS_SOR"]
        Int? INDEL_VQSR_max_gaussians_override = 4
        Int? INDEL_VQSR_mem_gb_override
        Int? SNP_VQSR_max_gaussians_override = 6
        Int? SNP_VQSR_mem_gb_override
        String query_project = project_id
        String destination_project = project_id
        String destination_dataset = dataset_name
        String fq_temp_table_dataset = "~{destination_project}.~{destination_dataset}"
        Array[String]? query_labels
        File? sample_names_to_extract
        File interval_weights_bed = "gs://broad-public-datasets/gvs/weights/gvs_vet_weights_1kb.bed"
        String extract_output_file_base_name = filter_set_name
        Int? extract_maxretries_override
        Int? extract_preemptible_override
        String? extract_output_gcs_dir
        Int? split_intervals_disk_size_override
        Int? split_intervals_mem_override
        Boolean extract_do_not_filter_override = false






## Your VCF files are ready!

The sharded VCF output files are listed in the `ExtractTask.output_vcf` workflow output, and the associated index files are listed in `ExtractTask.output_vcf_index`.


1. Get list of samples in sample set and load them into your fresh Terra workspace

2. Edit and run the notebook to validate your input data


The required input params are:





If your samples have not been reblocked
(y'all, why is the only reblocking WDL I see for our use case seemingly an AoU specific one?)










Sometimes the reblocking will fail for a small number of samples, so not all of the samples in the sample set with have the attribute "reblocked\_gvcf\_path". This wdl works on a sample set which means it is passed arrays of attributes all of which need to be filled in. Therefore, we need to create a sample set for loading with the subset of samples that have reblocked gvcfs. To do this, use the notebook `CreateSampleSetForGVSImport-paginated.ipynb`

### CreateSampleSetForGVSImport-paginated.ipynb

1. Get list of samples in sample set

	Currently we are not able to get the list of samples for a sampmle set from the fapi. Instead, go to the data tab, select the sample set you want to process and download the tsv. Unzip the sample set, remove the first line of the membership file (header info) and then extract the sample name column: `cut -f2 sample_set_membership.tsv > samples.txt`


	In the notebook tab, select this notebook and then edit. After the kernel starts, click the jupyter icon to open the filesystem. Navigate to the notebook->edit directory and upload the samples.txt file.
	
2. Edit and run the notebook
	
	Go back to the notebook and set the values in the 3rd box. Set `samples_list_file` to the name of the file you just uploaded. Set `sample_set_name` to the name of the existing sample set you are processing. 
	
	Run each cell of the notebook. Confirm a new sample set with `_gvsload_1` was created.
		
### Run the wdl

Currently the wdl is being imported from the methods repo [here](https://portal.firecloud.org/#methods/andrea_methods/ImportGenomes/68).
