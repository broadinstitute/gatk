# Configuration file for submitting Mutect2 jobs to SGE (call-caching disabled). Functional as of 12/15/16.
webservice {
  port = 8000
  interface = 0.0.0.0
  instance.name = "reference"
}

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  actor {
    default-dispatcher {
      fork-join-executor {
        # Number of threads = min(parallelism-factor * cpus, parallelism-max)
        # Below are the default values set by Akka, uncomment to tune these

        #parallelism-factor = 3.0
        #parallelism-max = 64
      }
    }
  }

  dispatchers {
    # A dispatcher for actors performing blocking io operations
    # Prevents the whole system from being slowed down when waiting for responses from external resources for instance
    io-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      # Using the forkjoin defaults, this can be tuned if we wish
    }

    # A dispatcher for actors handling API operations
    # Keeps the API responsive regardless of the load of workflows being run
    api-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher for engine actors
    # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs
    # on its own dispatcher to prevent backends from affecting its performance.
    engine-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # A dispatcher used by supported backend actors
    backend-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
    }

    # Note that without further configuration, all other actors run on the default dispatcher
  }
}

spray.can {
  server {
    request-timeout = 40s
  }
  client {
    request-timeout = 40s
    connecting-timeout = 40s
  }
}

system {
  // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting
  abort-jobs-on-terminate = false

  // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend
  max-retries = 10

  // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows
  workflow-restart = true

  // Cromwell will cap the number of running workflows at N
  max-concurrent-workflows = 5000

  // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  max-workflow-launch-count = 50

  // Number of seconds between workflow launches
  new-workflow-poll-rate = 20

  // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers
  number-of-workflow-log-copy-workers = 10
}

workflow-options {
  // These workflow options will be encrypted when stored in the database
  encrypted-fields: []

  // AES-256 key to use to encrypt the values in `encrypted-fields`
  base64-encryption-key: "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="

  // Directory where to write per workflow logs
  workflow-log-dir: "cromwell-workflow-logs"

  // When true, per workflow logs will be deleted after copying
  workflow-log-temporary: true

  // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.
  // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:
  //workflow-failure-mode: "ContinueWhilePossible"
}

// Optional call-caching configuration. Disabled by default.
call-caching {
  enabled = false

  // The Docker image specified in the 'runtime' section of a task can be used as-is
  // or Cromwell can lookup this Docker image to get a complete hash.  For example,
  // if a task specifies docker: "ubuntu:latest" and if lookup-docker-hash is true,
  // Then Cromwell would query DockerHub to resolve "ubuntu:latest" to something like
  // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47
  //
  // A value of 'true' means that call hashes will more accurately represent the
  // Docker image that was used to run the call, but at a cost of having to make a
  // request to an external service (DockerHub, GCR).  If a call fails to lookup a
  // Docker hash, it will fail.
  lookup-docker-hash = false
}

engine {
  // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.
  // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.
  // If you intend to be able to run workflows with this kind of declarations:
  // workflow {
  //    String str = read_string("gs://bucket/my-file.txt")
  // }
  // You will need to provide the engine with a gcs filesystem
  // Note that the default filesystem (local) is always available.
  //filesystems {
  //  gcs {
  //    auth = "application-default"
  //  }
  //}
}

backend {
  default = "SGE"
  providers {
    Local {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        run-in-background = true
        runtime-attributes = "String? docker"
        submit = "/bin/bash ${script}"
        submit-docker = "docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"

        // Root directory where Cromwell writes job results.  This directory must be
        // visible and writeable by the Cromwell process as well as the jobs that Cromwell
        // launches.
        root: "cromwell-executions"

        filesystems {
          local {
            // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs
            // The following are strategies used to make those links.  They are ordered.  If one fails
            // The next one is tried:
            //
            // hard-link: attempt to create a hard-link to the file
            // copy: copy the file
            // soft-link: create a symbolic link to the file
            //
            // NOTE: soft-link will be skipped for Docker jobs
            localization: [
              "hard-link", "soft-link", "copy"
            ]
          }
        }
      }
    }

    SGE {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        runtime-attributes = """
        Int cpu = 1
        Int memory_gb = 8
        String? sge_queue
        String? sge_project
        """
    
        submit = """
        qsub \
        -terse \
        -V \
        -b n \
	-l virtual_free=8g \
        -N ${job_name} \
        -wd ${cwd} \
        -o ${out} \
        -e ${err} \
        ${script}
        """
    
        kill = "qdel ${job_id}"
        check-alive = "qstat -j ${job_id}"
        job-id-regex = "(\\d+)"
      }
    }

    //LSF {
    //  actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
    //  config {
    //    submit = "bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /bin/bash ${script}"
    //    kill = "bkill ${job_id}"
    //    check-alive = "bjobs ${job_id}"
    //    job-id-regex = "Job <(\\d+)>.*"
    //  }
    //}

    // Example backend that _only_ runs workflows that specify docker for every command.
    //Docker {
    //  actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
    //  config {
    //    run-in-background = true
    //    runtime-attributes = "String docker"
    //    submit-docker = "docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"
    //  }
    //}

    //    HtCondor {
    //      actor-factory = "cromwell.backend.impl.htcondor.HtCondorBackendFactory"
    //      config {
    //        // Root directory where Cromwell writes job results.  This directory must be
    //        // visible and writeable by the Cromwell process as well as the jobs that Cromwell
    //        // launches.
    //        root: "cromwell-executions"
    //
    //        cache {
    //          provider = "cromwell.backend.impl.htcondor.caching.provider.mongodb.MongoCacheActorFactory"
    //          enabled = true
    //          forceRewrite = false
    //          db {
    //            host = "127.0.0.1"
    //            port = 27017
    //            name = "htcondor"
    //            collection = "cache"
    //          }
    //        }
    //
    //        filesystems {
    //          local {
    //            // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs
    //            // The following are strategies used to make those links.  They are ordered.  If one fails
    //            // The next one is tried:
    //            //
    //            // hard-link: attempt to create a hard-link to the file
    //            // copy: copy the file
    //            // soft-link: create a symbolic link to the file
    //            //
    //            // NOTE: soft-link will be skipped for Docker jobs
    //            localization: [
    //              "hard-link", "soft-link", "copy"
    //            ]
    //          }
    //        }
    //      }
    //    }

    //JES {
    //  actor-factory = "cromwell.backend.impl.jes.JesBackendLifecycleActorFactory"
    //  config {
    //    // Google project
    //    project = "my-cromwell-workflows"
    //
    //    // Base bucket for workflow executions
    //    root = "gs://my-cromwell-workflows-bucket"
    //
    //    // Polling for completion backs-off gradually for slower-running jobs.
    //    // This is the maximum polling interval (in seconds):
    //    maximum-polling-interval = 600
    //
    //    // Optional Dockerhub Credentials. Can be used to access private docker images.
    //    dockerhub {
    //      // account = ""
    //      // token = ""
    //    }
    //
    //    genomics {
    //      // A reference to an auth defined in the `google` stanza at the top.  This auth is used to create
    //      // Pipelines and manipulate auth JSONs.
    //      auth = "application-default"
    //      // Endpoint for APIs, no reason to change this unless directed by Google.
    //      endpoint-url = "https://genomics.googleapis.com/"
    //    }
    //
    //    filesystems {
    //      gcs {
    //        // A reference to a potentially different auth for manipulating files via engine functions.
    //        auth = "application-default"
    //      }
    //    }
    //  }
    //}

  }
}

services {
  KeyValue {
    class = "cromwell.services.keyvalue.impl.SqlKeyValueServiceActor"
  }
  MetadataService {
    class = "cromwell.services.metadata.impl.MetadataServiceActor"
  }
}

database {

    hsqldb {
      driver = "slick.driver.HsqldbDriver$"
      db {
        url = "jdbc:hsqldb:mem:${slick.uniqueSchema};shutdown=false;hsqldb.tx=mvcc"
        driver = "org.hsqldb.jdbcDriver"
        connectionTimeout = 1000
        slick.createSchema = true
      }
    }

    //  driver = "slick.driver.MySQLDriver$"
    //  db {
    //  url = "jdbc:mysql://mysql-prd2:3306/cromwellCaching?useSSL=false"
    //  user = "cromwellCaching"
    //  password = "cromwellCaching"
    //  driver = "com.mysql.jdbc.Driver"
    //  connectionTimeout = 5000 // NOTE: The default 1000ms is often too short for production //  mysql use
    //  }
      
  //migration{
  // read-batch-size= 1000
  //write-batch-size=1000
  //}
}
