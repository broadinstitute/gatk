@article{handsaker_large_2015,
	title = {Large multiallelic copy number variations in humans},
	volume = {47},
	copyright = {2015 Nature Publishing Group},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/ng.3200},
	doi = {10.1038/ng.3200},
	abstract = {Thousands of genomic segments appear to be present in widely varying copy numbers in different human genomes. We developed ways to use increasingly abundant whole-genome sequence data to identify the copy numbers, alleles and haplotypes present at most large multiallelic CNVs (mCNVs). We analyzed 849 genomes sequenced by the 1000 Genomes Project to identify most large ({\textgreater}5-kb) mCNVs, including 3,878 duplications, of which 1,356 appear to have 3 or more segregating alleles. We find that mCNVs give rise to most human variation in gene dosage—seven times the combined contribution of deletions and biallelic duplications—and that this variation in gene dosage generates abundant variation in gene expression. We describe 'runaway duplication haplotypes' in which genes, including HPR and ORM1, have mutated to high copy number on specific haplotypes. We also describe partially successful initial strategies for analyzing mCNVs via imputation and provide an initial data resource to support such analyses.},
	language = {en},
	number = {3},
	urldate = {2018-07-31},
	journal = {Nature Genetics},
	author = {Handsaker, Robert E. and Doren, Vanessa Van and Berman, Jennifer R. and Genovese, Giulio and Kashin, Seva and Boettger, Linda M. and McCarroll, Steven A.},
	month = mar,
	year = {2015},
	pages = {296--303},
	file = {Full Text PDF:/Users/asmirnov/Zotero/storage/FYN7WQCK/Handsaker et al. - 2015 - Large multiallelic copy number variations in human.pdf:application/pdf;Snapshot:/Users/asmirnov/Zotero/storage/9TPC8PTF/ng.html:text/html}
}

@article{klambauer_cn.mops:_2012,
	title = {cn.{MOPS}: mixture of {Poissons} for discovering copy number variations in next-generation sequencing data with a low false discovery rate},
	volume = {40},
	issn = {1362-4962},
	shorttitle = {cn.{MOPS}},
	doi = {10.1093/nar/gks003},
	abstract = {Quantitative analyses of next-generation sequencing (NGS) data, such as the detection of copy number variations (CNVs), remain challenging. Current methods detect CNVs as changes in the depth of coverage along chromosomes. Technological or genomic variations in the depth of coverage thus lead to a high false discovery rate (FDR), even upon correction for GC content. In the context of association studies between CNVs and disease, a high FDR means many false CNVs, thereby decreasing the discovery power of the study after correction for multiple testing. We propose 'Copy Number estimation by a Mixture Of PoissonS' (cn.MOPS), a data processing pipeline for CNV detection in NGS data. In contrast to previous approaches, cn.MOPS incorporates modeling of depths of coverage across samples at each genomic position. Therefore, cn.MOPS is not affected by read count variations along chromosomes. Using a Bayesian approach, cn.MOPS decomposes variations in the depth of coverage across samples into integer copy numbers and noise by means of its mixture components and Poisson distributions, respectively. The noise estimate allows for reducing the FDR by filtering out detections having high noise that are likely to be false detections. We compared cn.MOPS with the five most popular methods for CNV detection in NGS data using four benchmark datasets: (i) simulated data, (ii) NGS data from a male HapMap individual with implanted CNVs from the X chromosome, (iii) data from HapMap individuals with known CNVs, (iv) high coverage data from the 1000 Genomes Project. cn.MOPS outperformed its five competitors in terms of precision (1-FDR) and recall for both gains and losses in all benchmark data sets. The software cn.MOPS is publicly available as an R package at http://www.bioinf.jku.at/software/cnmops/ and at Bioconductor.},
	language = {eng},
	number = {9},
	journal = {Nucleic Acids Research},
	author = {Klambauer, G{\"u}nter and Schwarzbauer, Karin and Mayr, Andreas and Clevert, Djork-Arn{\'e} and Mitterecker, Andreas and Bodenhofer, Ulrich and Hochreiter, Sepp},
	month = may,
	year = {2012},
	pmid = {22302147},
	pmcid = {PMC3351174},
	keywords = {Chromosomes, Human, X, DNA Copy Number Variations, HapMap Project, High-Throughput Nucleotide Sequencing, Humans, Male, Poisson Distribution, Sequence Analysis, DNA, Software},
	pages = {e69}
}

@article{packer_clamms:_2016,
	title = {{CLAMMS}: a scalable algorithm for calling common and rare copy number variants from exome sequencing data},
	volume = {32},
	issn = {1367-4803},
	shorttitle = {{CLAMMS}},
	url = {https://academic.oup.com/bioinformatics/article/32/1/133/1743911},
	doi = {10.1093/bioinformatics/btv547},
	abstract = {Abstract.  Motivation: Several algorithms exist for detecting copy number variants (CNVs) from human exome sequencing read depth, but previous tools have not be},
	language = {en},
	number = {1},
	urldate = {2018-07-31},
	journal = {Bioinformatics},
	author = {Packer, Jonathan S. and Maxwell, Evan K. and O'Dushlaine, Colm and Lopez, Alexander E. and Dewey, Frederick E. and Chernomorsky, Rostislav and Baras, Aris and Overton, John D. and Habegger, Lukas and Reid, Jeffrey G.},
	month = jan,
	year = {2016},
	pages = {133--135},
	file = {Full Text PDF:/Users/asmirnov/Zotero/storage/KP4UZS6E/Packer et al. - 2016 - CLAMMS a scalable algorithm for calling common an.pdf:application/pdf;Snapshot:/Users/asmirnov/Zotero/storage/UMXBR2KN/1743911.html:text/html}
}

@article{ueda_deterministic_1998,
	title = {Deterministic annealing {EM} algorithm},
	volume = {11},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608097001330},
	doi = {10.1016/S0893-6080(97)00133-0},
	abstract = {This paper presents a deterministic annealing EM (DAEM) algorithm for maximum likelihood estimation problems to overcome a local maxima problem associated with the conventional EM algorithm. In our approach, a new posterior parameterized by `temperature' is derived by using the principle of maximum entropy and is used for controlling the annealing process. In the DAEM algorithm, the EM process is reformulated as the problem of minimizing the thermodynamic free energy by using a statistical mechanics analogy. Since this minimization is deterministically performed at each temperature, the total search is executed far more efficiently than in the simulated annealing. Moreover, the derived DAEM algorithm, unlike the conventional EM algorithm, can obtain better estimates free of the initial parameter values. We also apply the DAEM algorithm to the training of probabilistic neural networks using mixture models to estimate the probability density and demonstrate the performance of the DAEM algorithm.},
	number = {2},
	urldate = {2018-07-31},
	journal = {Neural Networks},
	author = {Ueda, Naonori and Nakano, Ryohei},
	month = mar,
	year = {1998},
	keywords = {Deterministic annealing, EM algorithm, Maximum entropy principle, Maximum likelihood estimation, Probabilistic neural networks},
	pages = {271--282},
	file = {ScienceDirect Snapshot:/Users/asmirnov/Zotero/storage/RTY6KU5V/S0893608097001330.html:text/html}
}

@article{mckenna_genome_2010,
	title = {The {Genome} {Analysis} {Toolkit}: {A} {MapReduce} framework for analyzing next-generation {DNA} sequencing data},
	issn = {1088-9051, 1549-5469},
	shorttitle = {The {Genome} {Analysis} {Toolkit}},
	url = {http://genome.cshlp.org/content/early/2010/07/19/gr.107524.110},
	doi = {10.1101/gr.107524.110},
	abstract = {Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes Project, are already revolutionizing our understanding of genetic variation among individuals. However, the massive data sets generated by NGS - the 1000 Genome pilot alone includes nearly five terabases - make writing feature-rich, efficient and robust analysis tools difficult for even computationally sophisticated individuals. Indeed, many professionals are limited in the scope and the ease with which they can answer scientific questions by the complexity of accessing and manipulating the data produced by these machines. Here we discuss our Genome Analysis Toolkit (GATK), a structured programming framework designed to ease the development of efficient and robust analysis tools for next-generation DNA sequencers using the functional programming philosophy of MapReduce. The GATK provides a small but rich set of data access patterns that encompass the majority of analysis tool needs. Separating specific analysis calculations from common data management infrastructure enables us to optimize the GATK framework for correctness, stability, CPU and memory efficiency, and to enable distributed and shared memory parallelization. We highlight the capabilities of the GATK by describing the implementation and application of robust, scale-tolerant tools like coverage calculators and SNP calling. We conclude that the GATK programming framework enables developers and analysts to quickly and easily write efficient and robust NGS tools, many of which have already been incorporated into large-scale sequencing projects like the 1000 Genomes Project and The Cancer Genome Atlas.},
	language = {en},
	urldate = {2018-07-31},
	journal = {Genome Research},
	author = {McKenna, Aaron Henrik and Hanna, Matthew and Banks, Eric and Sivachenko, Andrey and Cibulskis, Kristian and Kernytsky, Andrew and Garimella, Kiran and Altshuler, David and Gabriel, Stacey and Daly, Mark and Depristo, Mark},
	month = jul,
	year = {2010},
	pmid = {20644199},
	pages = {gr.107524.110},
	file = {Full Text PDF:/Users/asmirnov/Zotero/storage/2J6LSPFS/McKenna et al. - 2010 - The Genome Analysis Toolkit A MapReduce framework.pdf:application/pdf;Snapshot:/Users/asmirnov/Zotero/storage/YGP85F9R/gr.107524.html:text/html}
}

@article{salvatier_probabilistic_2016,
	title = {Probabilistic programming in {Python} using {PyMC}3},
	volume = {2},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-55},
	doi = {10.7717/peerj-cs.55},
	abstract = {Probabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package.},
	language = {en},
	urldate = {2018-07-31},
	journal = {PeerJ Computer Science},
	author = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
	month = apr,
	year = {2016},
	pages = {e55},
	file = {Full Text PDF:/Users/asmirnov/Zotero/storage/T43WQCRR/Salvatier et al. - 2016 - Probabilistic programming in Python using PyMC3.pdf:application/pdf;Snapshot:/Users/asmirnov/Zotero/storage/DF8FZC73/cs-55.html:text/html}
}

@article{jiang_codex:_2015,
	title = {{CODEX}: a normalization and copy number variation detection method for whole exome sequencing},
	volume = {43},
	issn = {1362-4962},
	shorttitle = {{CODEX}},
	doi = {10.1093/nar/gku1363},
	abstract = {High-throughput sequencing of DNA coding regions has become a common way of assaying genomic variation in the study of human diseases. Copy number variation (CNV) is an important type of genomic variation, but detecting and characterizing CNV from exome sequencing is challenging due to the high level of biases and artifacts. We propose CODEX, a normalization and CNV calling procedure for whole exome sequencing data. The Poisson latent factor model in CODEX includes terms that specifically remove biases due to GC content, exon capture and amplification efficiency, and latent systemic artifacts. CODEX also includes a Poisson likelihood-based recursive segmentation procedure that explicitly models the count-based exome sequencing data. CODEX is compared to existing methods on a population analysis of HapMap samples from the 1000 Genomes Project, and shown to be more accurate on three microarray-based validation data sets. We further evaluate performance on 222 neuroblastoma samples with matched normals and focus on a well-studied rare somatic CNV within the ATRX gene. We show that the cross-sample normalization procedure of CODEX removes more noise than normalizing the tumor against the matched normal and that the segmentation procedure performs well in detecting CNVs with nested structures.},
	language = {eng},
	number = {6},
	journal = {Nucleic Acids Research},
	author = {Jiang, Yuchao and Oldridge, Derek A. and Diskin, Sharon J. and Zhang, Nancy R.},
	month = mar,
	year = {2015},
	pmid = {25618849},
	pmcid = {PMC4381046},
	keywords = {Algorithms, Base Composition, Bias, Case-Control Studies, Databases, Nucleic Acid, DNA Copy Number Variations, DNA Helicases, DNA, Neoplasm, Exome, Female, High-Throughput Nucleotide Sequencing, Humans, Likelihood Functions, Male, Neuroblastoma, Nuclear Proteins, Sequence Analysis, DNA, X-linked Nuclear Protein},
	pages = {e39}
}

@article{olshen_circular_2004,
	title = {Circular binary segmentation for the analysis of array-based {DNA} copy number data},
	volume = {5},
	issn = {1465-4644},
	doi = {10.1093/biostatistics/kxh008},
	abstract = {DNA sequence copy number is the number of copies of DNA at a region of a genome. Cancer progression often involves alterations in DNA copy number. Newly developed microarray technologies enable simultaneous measurement of copy number at thousands of sites in a genome. We have developed a modification of binary segmentation, which we call circular binary segmentation, to translate noisy intensity measurements into regions of equal copy number. The method is evaluated by simulation and is demonstrated on cell line data with known copy number alterations and on a breast cancer cell line data set.},
	language = {eng},
	number = {4},
	journal = {Biostatistics (Oxford, England)},
	author = {Olshen, Adam B. and Venkatraman, E. S. and Lucito, Robert and Wigler, Michael},
	month = oct,
	year = {2004},
	pmid = {15475419},
	keywords = {Breast Neoplasms, Cell Line, Tumor, Computer Simulation, Female, Gene Dosage, Genome, Human, Humans, Oligonucleotide Array Sequence Analysis},
	pages = {557--572}
}

@article{fromer_discovery_2012,
	title = {Discovery and {Statistical} {Genotyping} of {Copy}-{Number} {Variation} from {Whole}-{Exome} {Sequencing} {Depth}},
	volume = {91},
	issn = {0002-9297},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3484655/},
	doi = {10.1016/j.ajhg.2012.08.005},
	abstract = {Sequencing of gene-coding regions (the exome) is increasingly used for studying human disease, for which copy-number variants (CNVs) are a critical genetic component. However, detecting copy number from exome sequencing is challenging because of the noncontiguous nature of the captured exons. This is compounded by the complex relationship between read depth and copy number; this results from biases in targeted genomic hybridization, sequence factors such as GC content, and batching of samples during collection and sequencing. We present a statistical tool (exome hidden Markov model [XHMM]) that uses principal-component analysis (PCA) to normalize exome read depth and a hidden Markov model (HMM) to discover exon-resolution CNV and genotype variation across samples. We evaluate performance on 90 schizophrenia trios and 1,017 case-control samples. XHMM detects a median of two rare ({\textless}1\%) CNVs per individual (one deletion and one duplication) and has 79\% sensitivity to similarly rare CNVs overlapping three or more exons discovered with microarrays. With sensitivity similar to state-of-the-art methods, XHMM achieves higher specificity by assigning quality metrics to the CNV calls to filter out bad ones, as well as to statistically genotype the discovered CNV in all individuals, yielding a trio call set with Mendelian-inheritance properties highly consistent with expectation. We also show that XHMM breakpoint quality scores enable researchers to explicitly search for novel classes of structural variation. For example, we apply XHMM to extract those CNVs that are highly likely to disrupt (delete or duplicate) only a portion of a gene.},
	number = {4},
	urldate = {2018-07-31},
	journal = {American Journal of Human Genetics},
	author = {Fromer, Menachem and Moran, Jennifer L. and Chambert, Kimberly and Banks, Eric and Bergen, Sarah E. and Ruderfer, Douglas M. and Handsaker, Robert E. and McCarroll, Steven A. and O'Donovan, Michael C. and Owen, Michael J. and Kirov, George and Sullivan, Patrick F. and Hultman, Christina M. and Sklar, Pamela and Purcell, Shaun M.},
	month = oct,
	year = {2012},
	pmid = {23040492},
	pmcid = {PMC3484655},
	pages = {597--607},
	file = {PubMed Central Full Text PDF:/Users/asmirnov/Zotero/storage/RFYAMXTZ/Fromer et al. - 2012 - Discovery and Statistical Genotyping of Copy-Numbe.pdf:application/pdf}
}

@article{zhang2009copy,
  title={Copy number variation in human health, disease, and evolution},
  author={Zhang, Feng and Gu, Wenli and Hurles, Matthew E and Lupski, James R},
  journal={Annual review of genomics and human genetics},
  volume={10},
  pages={451--481},
  year={2009},
  publisher={Annual Reviews}
}

@article{Kucukelbir:2017:ADV:3122009.3122023,
 author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
 title = {Automatic Differentiation Variational Inference},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2017},
 volume = {18},
 number = {1},
 month = jan,
 year = {2017},
 issn = {1532-4435},
 pages = {430--474},
 numpages = {45},
 url = {http://dl.acm.org/citation.cfm?id=3122009.3122023},
 acmid = {3122023},
 publisher = {JMLR.org},
 keywords = {Bayesian inference, approximate inference, probabilistic programming},
} 